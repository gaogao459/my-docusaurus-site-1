#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆçš„docs/get-startedç¿»è¯‘è„šæœ¬
- ä½¿ç”¨å¢å¼ºçš„æ¸…æ´—æœºåˆ¶
- æ·»åŠ æ’ç‰ˆç»Ÿä¸€åŒ–
- æ›´å¥½çš„é”™è¯¯å¤„ç†
"""

import os
import asyncio
import time
import json
import re
from pathlib import Path
import subprocess

# å¯¼å…¥ä¿®æ”¹åçš„ç¿»è¯‘æ¨¡å—
from Reflection_Workflow_XAI import translate_text

def preserve_frontmatter(content: str) -> tuple[str, str, str]:
    """
    åˆ†ç¦»frontmatterã€æ­£æ–‡å†…å®¹
    
    Returns:
        (frontmatter, body, full_content)
    """
    if content.startswith('---'):
        parts = content.split('---', 2)
        if len(parts) >= 3:
            frontmatter = f"---{parts[1]}---"
            body = parts[2].strip()
            return frontmatter, body, content
    
    return "", content.strip(), content

def clean_translation_artifacts(text: str) -> str:
    """å¢å¼ºç‰ˆæ¸…ç†ç¿»è¯‘è¿‡ç¨‹ä¸­çš„è¯´æ˜æ–‡å­—"""
    # ç§»é™¤ç¿»è¯‘è¯´æ˜æ®µè½ - æ‰©å±•æ›´å¤šæ¨¡å¼
    patterns = [
        # ä¸­æ–‡è¯´æ˜æ¨¡å¼
        r'ä»¥ä¸‹æ˜¯æ‚¨æä¾›çš„è‹±æ–‡æ–‡æœ¬çš„ä¸­æ–‡ç¿»è¯‘.*?(?=\n\n|\n#|\n\*|\Z)',
        r'è¿™ä¸ªç¿»è¯‘.*?(?=\n\n|\n#|\n\*|\Z)',
        r'ä»¥ä¸‹æ˜¯.*?ç¿»è¯‘.*?(?=\n\n|\n#|\n\*|\Z)',
        r'æ ¹æ®.*?å»ºè®®.*?ç¿»è¯‘.*?(?=\n\n|\n#|\n\*|\Z)',
        r'ä½œä¸º.*?è¯‘è€….*?(?=\n\n|\n#|\n\*|\Z)',
        
        # è‹±æ–‡è¯´æ˜æ¨¡å¼
        r'(?i)below is my.*?translation.*?(?=\n\n|\n#|\n\*|\Z)',
        r'(?i)here is.*?translation.*?(?=\n\n|\n#|\n\*|\Z)',
        r'(?i)i first analyzed.*?(?=\n\n|\n#|\n\*|\Z)',
        r'(?i)based on.*?reflection.*?(?=\n\n|\n#|\n\*|\Z)',
        r'(?i)as a professional translator.*?(?=\n\n|\n#|\n\*|\Z)',
        r'(?i)improvement suggestions.*?(?=\n\n|\n#|\n\*|\Z)',
        r'(?i)for each suggestion.*?(?=\n\n|\n#|\n\*|\Z)',
        r'(?i)sure,? here.*?(?=\n\n|\n#|\n\*|\Z)',
        
        # æ”¹è¿›å»ºè®®ç›¸å…³
        r'### Improvement Suggestions.*?(?=\n#|\Z)',
        r'### ä¼˜åŒ–è¯´æ˜.*?(?=\n#|\Z)',
        r'æ­¤ç‰ˆæœ¬çš„ç¿»è¯‘.*?(?=\n\n|\n#|\Z)',
        
        # å¤šä½™çš„åˆ†éš”ç¬¦
        r'---\n\n(?=# )',
        r'\n\n---\n\n(?=è¿™ä¸ªç¿»è¯‘)',
    ]
    
    cleaned = text
    for pattern in patterns:
        cleaned = re.sub(pattern, '', cleaned, flags=re.DOTALL | re.MULTILINE)
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    cleaned = cleaned.strip()
    
    return cleaned

def format_markdown(file_path: Path):
    """ä½¿ç”¨prettieræ ¼å¼åŒ–markdownæ–‡ä»¶"""
    try:
        result = subprocess.run(
            ["npx", "prettier", "--write", str(file_path), "--parser", "markdown"],
            capture_output=True,
            text=True,
            cwd=file_path.parent.parent.parent  # é¡¹ç›®æ ¹ç›®å½•
        )
        if result.returncode == 0:
            print(f"âœ… æ ¼å¼åŒ–å®Œæˆ: {file_path}")
        else:
            print(f"âš ï¸  æ ¼å¼åŒ–è­¦å‘Š: {file_path} - {result.stderr}")
    except Exception as e:
        print(f"âŒ æ ¼å¼åŒ–å¤±è´¥: {file_path} - {e}")

async def translate_markdown_file(file_path: Path, source_lang="English", target_lang="Chinese"):
    """ç¿»è¯‘å•ä¸ªmarkdownæ–‡ä»¶"""
    try:
        print(f"\n{'='*60}")
        print(f"ç¿»è¯‘æ–‡ä»¶: {file_path}")
        print(f"{'='*60}")
        
        # è¯»å–åŸæ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        # åˆ†ç¦»frontmatterå’Œæ­£æ–‡
        frontmatter, body, _ = preserve_frontmatter(original_content)
        
        if not body.strip():
            print(f"è·³è¿‡ç©ºæ–‡ä»¶: {file_path}")
            return True
        
        print(f"åŸæ–‡å†…å®¹é¢„è§ˆ: {body[:200]}...")
        
        # ä½¿ç”¨Reflection Workflowç¿»è¯‘æ­£æ–‡
        start_time = time.time()
        result = await translate_text(
            source_text=body,
            source_lang=source_lang,
            target_lang=target_lang,
            model="grok-3-mini"
        )
        translation_time = time.time() - start_time
        
        # è·å–ç¿»è¯‘ç»“æœå¹¶æ¸…ç†
        translated_body = result["final_translation"]
        cleaned_translation = clean_translation_artifacts(translated_body)
        
        # æ£€æŸ¥æ¸…ç†æ•ˆæœ
        if any(keyword in cleaned_translation.lower() for keyword in 
               ['below is', 'here is', 'improvement suggestions', 'æ”¹è¿›å»ºè®®']):
            print(f"âš ï¸  è­¦å‘Š: æ¸…ç†åä»å¯èƒ½åŒ…å«è¯´æ˜æ–‡å­—")
        
        # é‡æ–°ç»„åˆå†…å®¹
        final_content = frontmatter + "\n\n" + cleaned_translation if frontmatter else cleaned_translation
        
        # å†™å…¥ç¿»è¯‘ç»“æœï¼Œè¦†ç›–åŸæ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(final_content)
        
        # æ ¼å¼åŒ–æ–‡ä»¶
        format_markdown(file_path)
        
        print(f"âœ… ç¿»è¯‘å®Œæˆ: {file_path}")
        print(f"â±ï¸  ç”¨æ—¶: {translation_time:.2f}ç§’")
        print(f"ğŸ“Š ç»Ÿè®¡: {result['stats']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å¤±è´¥: {file_path}")
        print(f"é”™è¯¯: {str(e)}")
        return False

async def translate_directory(base_dir: Path, source_lang="English", target_lang="Chinese"):
    """é€’å½’ç¿»è¯‘ç›®å½•ä¸‹çš„æ‰€æœ‰markdownæ–‡ä»¶"""
    
    print(f"\nğŸš€ å¼€å§‹ç¿»è¯‘ {base_dir} ç›®å½•")
    print(f"æºè¯­è¨€: {source_lang} -> ç›®æ ‡è¯­è¨€: {target_lang}")
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„markdownæ–‡ä»¶
    md_files = []
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.md') and not file.startswith('_'):
                file_path = Path(root) / file
                md_files.append(file_path)
    
    if not md_files:
        print(f"âŒ åœ¨ {base_dir} ä¸­æœªæ‰¾åˆ°markdownæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(md_files)} ä¸ªmarkdownæ–‡ä»¶")
    for f in md_files:
        print(f"  - {f}")
    
    # ç¿»è¯‘æ‰€æœ‰æ–‡ä»¶
    total_start = time.time()
    success_count = 0
    
    for i, file_path in enumerate(md_files, 1):
        print(f"\n[{i}/{len(md_files)}] å¤„ç†æ–‡ä»¶...")
        
        try:
            success = await translate_markdown_file(file_path, source_lang, target_lang)
            if success:
                success_count += 1
            
            # åœ¨æ–‡ä»¶ä¹‹é—´æ·»åŠ å°å»¶è¿Ÿ
            if i < len(md_files):
                await asyncio.sleep(1)
                
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            continue
    
    total_time = time.time() - total_start
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ğŸ‰ ç¿»è¯‘ä»»åŠ¡å®Œæˆ!")
    print(f"{'='*60}")
    print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {len(md_files)}")
    print(f"âœ… æˆåŠŸç¿»è¯‘: {success_count}")
    print(f"âŒ å¤±è´¥: {len(md_files) - success_count}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_count/len(md_files)*100:.1f}%")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
    print(f"âš¡ å¹³å‡é€Ÿåº¦: {total_time/len(md_files):.1f}ç§’/æ–‡ä»¶")

async def main():
    """ä¸»å‡½æ•°"""
    try:
        # è®¾ç½®ç›®æ ‡ç›®å½•
        docs_dir = Path("docs/get-started")
        
        if not docs_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
            return
        
        # ç¡®è®¤æ“ä½œ
        print(f"ğŸ”„ å³å°†ä½¿ç”¨æ”¹è¿›ç‰ˆç¿»è¯‘ {docs_dir} ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶")
        print("âš ï¸  è­¦å‘Š: è¿™å°†ç›´æ¥è¦†ç›–åŸæ–‡ä»¶!")
        print("âœ¨ æ–°ç‰¹æ€§: å¢å¼ºæ¸…æ´— + è‡ªåŠ¨æ ¼å¼åŒ–")
        
        # å¼€å§‹ç¿»è¯‘
        await translate_directory(docs_dir)
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main()) 